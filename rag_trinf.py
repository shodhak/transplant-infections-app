import os
import fitz  # PyMuPDF
import numpy as np
import openai
import pandas as pd
from pydantic import BaseModel
from fastapi import FastAPI
from starlette.responses import JSONResponse
from typing import List, Optional
import faiss
from sentence_transformers import SentenceTransformer

# FastAPI initialization
app = FastAPI()
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
vector_store = None
pdf_path = "merged_papers.pdf"
FAISS_INDEX_PATH = "faiss_index/index"

# Simple text splitter function
def split_text(text, chunk_size=1500, overlap=100):
    """Split text into chunks"""
    chunks = []
    start = 0
    text_len = len(text)
    
    while start < text_len:
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap
    
    return chunks

def extract_content_from_pdf():
    """Extract text from PDF file."""
    if not os.path.exists(pdf_path):
        print("âŒ Error: PDF document not found.")
        return {"raw_text": "", "tables": []}

    print("ðŸ”¹ Opening PDF document...")
    doc = fitz.open(pdf_path)
    raw_text = []

    for page_index, page in enumerate(doc):
        print(f"ðŸ”¹ Processing page {page_index + 1}...")
        raw_text.append(page.get_text("text"))

    full_text = "\n".join(raw_text)
    return {"raw_text": full_text, "tables": []}

FAISS_INDEX_PATH = "faiss_index/index"  # Update to new path

def create_faiss_index(data):
    global vector_store
    texts = split_text(data["raw_text"])
    print(f"Total chunks created: {len(texts)}")
    embeddings = model.encode(texts)
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings.astype('float32'))
    vector_store = {'index': index, 'texts': texts}
    # Save index
    faiss.write_index(index, f"{FAISS_INDEX_PATH}.faiss")
    import pickle
    with open(f"{FAISS_INDEX_PATH}.pkl", 'wb') as f:
        pickle.dump(texts, f)
    print("âœ… FAISS index created successfully!")

def load_document(force_recreate=False):
    global vector_store
    index_exists = os.path.exists(f"{FAISS_INDEX_PATH}.faiss")
    texts_exists = os.path.exists(f"{FAISS_INDEX_PATH}.pkl")
    if index_exists and texts_exists and not force_recreate:
        print("ðŸ”„ Loading existing FAISS index...")
        try:
            index = faiss.read_index(f"{FAISS_INDEX_PATH}.faiss")
            import pickle
            with open(f"{FAISS_INDEX_PATH}.pkl", 'rb') as f:
                texts = pickle.load(f)
            if not texts or index.ntotal == 0:
                print("âŒ FAISS index or text chunks are empty. Rebuilding index...")
                raise ValueError("Empty index or text chunks")
            vector_store = {'index': index, 'texts': texts}
            print("âœ… FAISS index loaded successfully!")
            return True
        except Exception as e:
            print(f"âŒ Error loading FAISS index: {e}. Rebuilding index.")
            if index_exists:
                os.remove(f"{FAISS_INDEX_PATH}.faiss")
            if texts_exists:
                os.remove(f"{FAISS_INDEX_PATH}.pkl")
    print("âš ï¸ Processing the document and building FAISS index...")
    extracted_data = extract_content_from_pdf()
    if extracted_data["raw_text"]:
        create_faiss_index(extracted_data)
        return True
    else:
        print("âŒ Could not process PDF or extract text.")
        vector_store = None
        return False

class ChatRequest(BaseModel):
    query: str
    history: Optional[List[dict]] = []

def query_openai(context, query):
    """Query OpenAI GPT-4o."""
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    if not OPENAI_API_KEY:
        return "Error: OpenAI API key is missing."
    
    client = openai.OpenAI(api_key=OPENAI_API_KEY)
    
    system_message = (
        "You are a clinician scientist in transplant infections. "
        "Answer questions based on your expertise with the publications. "
        "Make sure that all information you provide is accurate."
    )
    
    prompt = f"Context: {context}\n\nQuestion: {query}\nAnswer:"
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error: {e}"

@app.get("/")
def root():
    return {"message": "FastAPI is running! Go to /docs to test the API."}

@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "vector_store_loaded": vector_store is not None,
        "openai_key_set": bool(os.getenv("OPENAI_API_KEY"))
    }

@app.post("/chat/")
def query_api_post(chat_request: ChatRequest):
    """Process user queries."""
    print(f"Received query: {chat_request.query}")
    
    try:
        if vector_store is None:
            load_document()
            if vector_store is None:
                return JSONResponse(content={"error": "FAISS index not loaded"}, status_code=500)
        
        # Search similar texts
        query_embedding = model.encode([chat_request.query])
        D, I = vector_store['index'].search(query_embedding.astype('float32'), k=3)
        
        # Get relevant texts
        relevant_texts = [vector_store['texts'][i] for i in I[0]]
        context = "\n".join(relevant_texts)
        
        answer = query_openai(context, chat_request.query)
        
        return JSONResponse(content={"answer": answer})
    except Exception as e:
        print(f"âŒ Error: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)

# Try to load on startup
try:
    load_document()
except Exception as e:
    print(f"âš ï¸ Startup load failed: {e}")
